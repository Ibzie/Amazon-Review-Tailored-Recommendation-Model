{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    "    .set(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/amazon.reviews\") \\\n",
    "    .set(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/amazon.reviews\") \\\n",
    "    .set(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .set(\"spark.kryoserializer.buffer.max\", \"512m\") # set the Kryo serialization buffer size\n",
    "\n",
    "# create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MongoDB to PySpark\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-------+--------------------+--------------+--------+\n",
      "|                 _id|      asin|overall|             summary|unixReviewTime|verified|\n",
      "+--------------------+----------+-------+--------------------+--------------+--------+\n",
      "|{645781fe5f85f503...|B017O9P72A|    1.0|very buggy doesnt...|    1449792000|   false|\n",
      "|{645781fe5f85f503...|B017O9P72A|    4.0|      so far so good|    1449532800|   false|\n",
      "|{645781fe5f85f503...|B017O9P72A|    1.0|         time waster|    1449446400|   false|\n",
      "|{645781fe5f85f503...|B017O9P72A|    2.0|               buggy|    1449273600|   false|\n",
      "|{645781fe5f85f503...|B017O9P72A|    1.0|     stopped working|    1517529600|   false|\n",
      "|{645781fe5f85f503...|B017O9P72A|    5.0|               great|    1515974400|   false|\n",
      "|{645781fe5f85f503...|B017O9P72A|    1.0|        returning to|    1515110400|   false|\n",
      "|{645781fe5f85f503...|B017O9P72A|    1.0|can not connect t...|    1515024000|   false|\n",
      "|{645781fe5f85f503...|B017O9P72A|    1.0|connecting is a n...|    1514592000|   false|\n",
      "|{645781fe5f85f503...|B017O9P72A|    1.0|       does not work|    1514505600|   false|\n",
      "+--------------------+----------+-------+--------------------+--------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "\n",
    "# Show the first 10 rows of the DataFrame\n",
    "df.show(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+----------+-------+-----------------------+--------------+--------+-----------------+\n",
      "|_id                       |asin      |overall|summary                |unixReviewTime|verified|processed_summary|\n",
      "+--------------------------+----------+-------+-----------------------+--------------+--------+-----------------+\n",
      "|{645781fe5f85f503f9406617}|B017O9P72A|1.0    |very buggy doesnt work |1449792000    |0       |[]               |\n",
      "|{645781fe5f85f503f9406618}|B017O9P72A|4.0    |so far so good         |1449532800    |0       |[good]           |\n",
      "|{645781fe5f85f503f9406619}|B017O9P72A|1.0    |time waster            |1449446400    |0       |[]               |\n",
      "|{645781fe5f85f503f940661a}|B017O9P72A|2.0    |buggy                  |1449273600    |0       |[]               |\n",
      "|{645781fe5f85f503f940661b}|B017O9P72A|1.0    |stopped working        |1517529600    |0       |[]               |\n",
      "|{645781fe5f85f503f940661c}|B017O9P72A|5.0    |great                  |1515974400    |0       |[great]          |\n",
      "|{645781fe5f85f503f940661d}|B017O9P72A|1.0    |returning to           |1515110400    |0       |[]               |\n",
      "|{645781fe5f85f503f940661e}|B017O9P72A|1.0    |can not connect to echo|1515024000    |0       |[]               |\n",
      "|{645781fe5f85f503f940661f}|B017O9P72A|1.0    |connecting is a no go  |1514592000    |0       |[]               |\n",
      "|{645781fe5f85f503f9406620}|B017O9P72A|1.0    |does not work          |1514505600    |0       |[]               |\n",
      "+--------------------------+----------+-------+-----------------------+--------------+--------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import udf, when\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define the two lists\n",
    "positive_words  = [\"excellent\", \"awesome\", \"effective\", \"outstanding\", \"amazing\", \"good\", \"great\", \"love\", \"perfect\", \"devotion\"]\n",
    "negative_words  = [\"terrible\", \"poor\", \"disappointing\", \"awful\", \"horrible\", \"dreadful\", \"unsatisfactory\", \"subpar\", \"lousy\", \"inferior\"]\n",
    "\n",
    "# Define a UDF to tokenize, lowercase, and remove stop words while also filtering based on the two lists\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "def preprocess_summary(summary):\n",
    "    tokens = nltk.tokenize.word_tokenize(summary.lower())\n",
    "    tokens = [token for token in tokens if token not in stopwords and (token in positive_words or token in negative_words)]\n",
    "    return tokens\n",
    "\n",
    "preprocess_summary_udf = udf(preprocess_summary, ArrayType(StringType()))\n",
    "df = df.withColumn(\"verified\", when(df[\"verified\"]==\"true\", 1).otherwise(0))\n",
    "# Apply the UDF to the \"summary\" column and create a new column \"processed_summary\"\n",
    "df = df.withColumn(\"processed_summary\", preprocess_summary_udf(df['summary']))\n",
    "\n",
    "# Show the first 10 rows of the processed summary column\n",
    "df.show(10, False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-------+--------------------+--------------+--------+-----------------+\n",
      "|                 _id|      asin|overall|             summary|unixReviewTime|verified|processed_summary|\n",
      "+--------------------+----------+-------+--------------------+--------------+--------+-----------------+\n",
      "|{645781fe5f85f503...|B017O9P72A|    1.0|very buggy doesnt...|    1449792000|       0|               []|\n",
      "|{645781fe5f85f503...|B017O9P72A|    4.0|      so far so good|    1449532800|       0|           [good]|\n",
      "|{645781fe5f85f503...|B017O9P72A|    1.0|         time waster|    1449446400|       0|               []|\n",
      "|{645781fe5f85f503...|B017O9P72A|    2.0|               buggy|    1449273600|       0|               []|\n",
      "|{645781fe5f85f503...|B017O9P72A|    1.0|     stopped working|    1517529600|       0|               []|\n",
      "|{645781fe5f85f503...|B017O9P72A|    5.0|               great|    1515974400|       0|          [great]|\n",
      "|{645781fe5f85f503...|B017O9P72A|    1.0|        returning to|    1515110400|       0|               []|\n",
      "|{645781fe5f85f503...|B017O9P72A|    1.0|can not connect t...|    1515024000|       0|               []|\n",
      "|{645781fe5f85f503...|B017O9P72A|    1.0|connecting is a n...|    1514592000|       0|               []|\n",
      "|{645781fe5f85f503...|B017O9P72A|    1.0|       does not work|    1514505600|       0|               []|\n",
      "+--------------------+----------+-------+--------------------+--------------+--------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[_id: struct<oid:string>, asin: string, overall: double, summary: string, unixReviewTime: int, verified: int, processed_summary: array<string>, id: bigint, asin_index: double]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "df = df.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "#StringIndexer transformer\n",
    "indexer = StringIndexer(inputCol=\"asin\", outputCol=\"asin_index\")\n",
    "indexed_df = indexer.fit(df).transform(df)\n",
    "print(indexed_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-------+--------------------+--------------+--------+-----------------+---+----------+\n",
      "|                 _id|      asin|overall|             summary|unixReviewTime|verified|processed_summary| id|asin_index|\n",
      "+--------------------+----------+-------+--------------------+--------------+--------+-----------------+---+----------+\n",
      "|{645781fe5f85f503...|B017O9P72A|    1.0|very buggy doesnt...|    1449792000|       0|               []|  0|  241721.0|\n",
      "|{645781fe5f85f503...|B017O9P72A|    4.0|      so far so good|    1449532800|       0|           [good]|  1|  241721.0|\n",
      "|{645781fe5f85f503...|B017O9P72A|    1.0|         time waster|    1449446400|       0|               []|  2|  241721.0|\n",
      "|{645781fe5f85f503...|B017O9P72A|    2.0|               buggy|    1449273600|       0|               []|  3|  241721.0|\n",
      "|{645781fe5f85f503...|B017O9P72A|    1.0|     stopped working|    1517529600|       0|               []|  4|  241721.0|\n",
      "|{645781fe5f85f503...|B017O9P72A|    5.0|               great|    1515974400|       0|          [great]|  5|  241721.0|\n",
      "|{645781fe5f85f503...|B017O9P72A|    1.0|        returning to|    1515110400|       0|               []|  6|  241721.0|\n",
      "|{645781fe5f85f503...|B017O9P72A|    1.0|can not connect t...|    1515024000|       0|               []|  7|  241721.0|\n",
      "|{645781fe5f85f503...|B017O9P72A|    1.0|connecting is a n...|    1514592000|       0|               []|  8|  241721.0|\n",
      "|{645781fe5f85f503...|B017O9P72A|    1.0|       does not work|    1514505600|       0|               []|  9|  241721.0|\n",
      "+--------------------+----------+-------+--------------------+--------------+--------+-----------------+---+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexed_df.show(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+--------+-----------------+---+----------+\n",
      "|overall|unixReviewTime|verified|processed_summary| id|asin_index|\n",
      "+-------+--------------+--------+-----------------+---+----------+\n",
      "|    1.0|    1449792000|       0|               []|  0|  241721.0|\n",
      "|    4.0|    1449532800|       0|           [good]|  1|  241721.0|\n",
      "|    1.0|    1449446400|       0|               []|  2|  241721.0|\n",
      "|    2.0|    1449273600|       0|               []|  3|  241721.0|\n",
      "|    1.0|    1517529600|       0|               []|  4|  241721.0|\n",
      "|    5.0|    1515974400|       0|          [great]|  5|  241721.0|\n",
      "|    1.0|    1515110400|       0|               []|  6|  241721.0|\n",
      "|    1.0|    1515024000|       0|               []|  7|  241721.0|\n",
      "|    1.0|    1514592000|       0|               []|  8|  241721.0|\n",
      "|    1.0|    1514505600|       0|               []|  9|  241721.0|\n",
      "+-------+--------------+--------+-----------------+---+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = indexed_df.drop(\"summary\", \"_id\",\"asin\")\n",
    "# df.write.parquet(\"data_for_training.parquet\")\n",
    "df.show(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+--------+-----------------+---+----------+\n",
      "|overall|unixReviewTime|verified|processed_summary| id|asin_index|\n",
      "+-------+--------------+--------+-----------------+---+----------+\n",
      "|    1.0|    1515024000|       0|               []|  7|  241721.0|\n",
      "|    1.0|    1510272000|       0|               []| 16|  241721.0|\n",
      "|    1.0|    1505260800|       0|               []| 18|  241721.0|\n",
      "|    1.0|    1482969600|       0|               []| 50|  241721.0|\n",
      "|    1.0|    1482710400|       0|               []| 58|  241721.0|\n",
      "|    4.0|    1481241600|       0|               []| 63|  241721.0|\n",
      "|    1.0|    1480982400|       0|               []| 66|  241721.0|\n",
      "|    5.0|    1477353600|       0|               []| 72|  241721.0|\n",
      "|    1.0|    1476921600|       0|               []| 79|  241721.0|\n",
      "|    5.0|    1472860800|       0|               []| 88|  241721.0|\n",
      "+-------+--------------+--------+-----------------+---+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample the original dataframe\n",
    "sampled_df = df.sample(withReplacement=False, fraction=0.1, seed=42)\n",
    "sampled_df.show(10)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Cast the \"id\" column to integer\n",
    "sampled_df = sampled_df.withColumn(\"id\", col(\"id\").cast(\"integer\"))\n",
    "\n",
    "# Split the data into training and test sets\n",
    "(training, test) = sampled_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Create the ALS model\n",
    "als = ALS(userCol=\"id\", itemCol=\"asin_index\", ratingCol=\"overall\", coldStartStrategy=\"drop\")\n",
    "\n",
    "# Fit the model to the training data\n",
    "model = als.fit(training)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Evaluate the model using RMSE\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"overall\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "\n",
    "userRecs = model.recommendForAllUsers(3)\n",
    "# Show the recommendations for the first user\n",
    "userRecs.filter(col(\"id\") == 0).show()\n",
    "print('yes')\n",
    "userRecs.write.parquet(\"user_recommendations.parquet\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
